% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/regmdc.R
\name{regmdc}
\alias{regmdc}
\title{Fit to data a nonparametric regression model with mixed derivative constraints}
\usage{
regmdc(
  X_design,
  y,
  s,
  method,
  V = Inf,
  threshold = 1e-06,
  is_scaled = FALSE,
  is_lattice = FALSE,
  is_monotonically_increasing = TRUE,
  is_totally_concave = TRUE,
  number_of_bins = NULL,
  increasing_covariates = NULL,
  decreasing_covariates = NULL,
  concave_covariates = NULL,
  convex_covariates = NULL,
  variation_constrained_covariates = NULL,
  extra_linear_covariates = NULL
)
}
\arguments{
\item{X_design}{A numeric design matrix. Each row corresponds to an
individual data.}

\item{y}{A numeric observation vector of a response variable.}

\item{s}{A numeric scalar indicating the maximum order of interaction between
covariates allowed in the estimation method.}

\item{method}{A string indicating the estimation method. One of "em", "hk",
"emhk", "tc", "mars", and "tcmars".}

\item{V}{A numeric scalar. An upper bound on complexity measure (variation)
in a scaled domain. Required for "hk" and "mars", and possibly used for
"emhk" and "tcmars".}

\item{threshold}{A numeric scalar to determine whether each component of the
solution to the LASSO problem is zero or not.}

\item{is_scaled}{A logical scalar for whether the design matrix is scaled so
that every entry is between 0 and 1. If \code{FALSE}, the min-max scaling is
applied to each column of the design matrix.}

\item{is_lattice}{A logical scalar for whether the design is a lattice or not.
Only used for "em", "hk", and "emhk". See details below.}

\item{is_monotonically_increasing}{A logical scalar for whether the method is
entirely monotonically increasing regression or entirely monotonically
decreasing regression. Only used for "em".}

\item{is_totally_concave}{A logical scalar for whether the method is totally
concave regression or totally convex regression. Only used for "tc".}

\item{number_of_bins}{An integer or an integer vector of the numbers of bins
for the approximate method. Currently available for "tc", "mars", and
"tcmars". See details below.}

\item{increasing_covariates}{An integer or a string vector of monotonically
increasing covariates. Possibly used for "em" and "emhk". See details below.}

\item{decreasing_covariates}{An integer or a string vector of monotonically
decreasing covariates. Possibly used for "em" and "emhk". See details below.}

\item{concave_covariates}{An integer or a string vector of concave covariates.
Possibly used for "tc" and "tcmars". See details below.}

\item{convex_covariates}{An integer or a string vector of convex covariates.
Possibly used for "tc" and "tcmars". See details below.}

\item{variation_constrained_covariates}{An integer or a string vector of
covariates whose variation is constrained. Possibly used for "emhk" and
"tcmars". See details below.}

\item{extra_linear_covariates}{An integer vector or a string vector of extra
linear covariates added to the model. Possibly used for "tc", "mars", and
"tcmars". See details below.}
}
\description{
Given an estimation method, this function builds a model by solving the
corresponding constrained LASSO problem. Available estimation methods are
entirely monotonic regression ("em"), Hardy—Krause variation denoising ("hk"),
their generalization ("emhk"), totally concave regression ("tc"), MARS via
LASSO ("mars"), and their generalization ("tcmars"). For details about the
corresponding LASSO problems, see, for example, Section 3 of Fang et al.
(2021) (for entirely monotonic regression and Hardy—Krause variation
denoising), Section 2 of Ki et al. (2024) (for MARS via LASSO), and Section 3
of Ki and Guntuboyina (2025+) (for totally concave regression).
}
\details{
You can also set \code{is_lattice} as \code{TRUE} even when the design is not a lattice.
If \code{is_lattice} is \code{TRUE}, the model is fitted in a faster way; whereas if it
is \code{FALSE}, the model is fitted in a memory-efficient way. Currently,
\code{is_lattice} can be used for entirely monotonic regression, Hardy—Krause
variation denoising, and their generalization.

The approximate method is used if \code{number_of_bins} is not \code{NULL}. Currently,
the approximate method is available for totally concave regression, MARS via
LASSO, and their generalization. You can put an integer into \code{number_of_bins}
if the numbers of bins for the approximate method are the same for all
covariates. If they are different, you need to put an integer vector of the
numbers of bins. The number of bins can be \code{NA} for some covariates. In that
case, the approximate method is not applied to such covariates.

Using \code{extra_linear_covariates}, you can add to the model extra covariates
that have linear relationship with the response variable. The interaction
between these covariates and the interaction between these covariates and the
other covariates are not considered in the model.

For entirely monotonic regression (resp. totally concave regression), you can
fit a model using both covariates that have a increasing (resp. concave)
relationship with the response variable and covariates that have a decreasing
(resp. convex) relationship with the response variable. You can utilize
\code{increasing_covariates} and \code{decreasing_covariates} for entirely monotonic
regression, and \code{concave_covariates} and \code{convex_covariates} for totally
concave regression.

For entirely monotonic regression, if both \code{increasing_covariates} and
\code{decreasing_covariates} are \code{NULL}, they are determined by
\code{is_monotonically_increasing}. For example, if \code{is_monotonically_increasing}
is \code{TRUE}, \code{increasing_covariates} and \code{decreasing_covariates} become the
whole set of covariates and \code{NULL}, respectively. If exactly one of
\code{increasing_covariates} and \code{decreasing_covariates} is not \code{NULL}, the one
that is \code{NULL} is changed to the complement of the other. If both of them are
not \code{NULL}, their union must be the whole set of covariates.

For totally concave regression, if both \code{concave_covariates} and
\code{convex_covariates} are \code{NULL}, they are determined by \code{is_totally_concave}.
For instance, if \code{is_totally_concave} is \code{TRUE}, \code{concave_covariates} is the
whole set of covariates while \code{convex_covariates} is \code{NULL}. If exactly one
of \code{concave_covariates} and \code{convex_covariates} is not \code{NULL}, the one that
is \code{NULL} is converted to the relative complement of the other in the
complement of \code{extra_linear_covariates}. If both of them are not \code{NULL}, their
union must include all covariates except those in \code{extra_linear_covariates}.

In the generalization of entirely monotonic regression and Hardy—Krause
variation denoising (resp. the generalization of totally concave regression
and MARS via LASSO), you can also have covariates whose variation is
constrained as in Hardy—Krause variation denoising (resp. MARS via LASSO).
You can specify those covariates using \code{variation_constrained_covariates}. If
\code{variation_constrained_covariates} is \code{NULL}, it is automatically set as the
whole set of covariates. If \code{variation_constrained_covariates} is not \code{NULL},
\code{increasing_covariates} and \code{decreasing_covariates} (resp. \code{concave_covariates}
and \code{convex_covariates}) are determined as in the case of entirely monotonic
regression (resp. MARS via LASSO). The only difference is that the covariates
in \code{variation_constrained_covariates} are excluded from the whole set of
covariates (resp. the complement of \code{extra_linear_covariates}) whenever it is
considered. If none of \code{variation_constrained_covariates},
\code{increasing_covariates}, and \code{decreasing_covariates} is \code{NULL}, there union
must be the whole set of covariates (resp. the complement of
\code{extra_linear_covariates}).

For \code{increasing_covariates}, \code{decreasing_covariates}, \code{concave_covariates},
\code{convex_covariates}, \code{variation_constrained_covariates}, and
\code{extra_linear_covariates}, you can put either an integer vector of column
indices or a string vector of column names.
}
\examples{
fstar <- function(x) {(
  (x[1] - 0.25 >= 0) + (x[2] - 0.25 >= 0)
  + (x[1] - 0.25 >= 0) * (x[2] - 0.25 >= 0)
)}  # the true underlying function
X_design <- expand.grid(rep(list(seq(0, 1, length.out = 5L)), 3L))
colnames(X_design) <- c("VarA", "VarB", "VarC")
theta <- apply(X_design, MARGIN = 1L, FUN = fstar)
sigma <- 0.1
y <- theta + sigma * rnorm(nrow(X_design))

regmdc(X_design, y, s = 1L, method = "em")
regmdc(X_design, y, s = 2L, method = "em")
regmdc(X_design, y, s = 2L, method = "em", is_scaled = TRUE)
regmdc(X_design, y, s = 2L, method = "em", is_lattice = TRUE)
regmdc(X_design, y, s = 2L, method = "em", is_monotonically_increasing = FALSE)
regmdc(X_design, y, s = 2L, method = "em", increasing_covariates = c(1L, 2L))
regmdc(X_design, y, s = 2L, method = "em", decreasing_covariates = c(3L))
regmdc(X_design, y, s = 2L, method = "em",
       increasing_covariates = c("VarA", "VarB"),
       decreasing_covariates = c("VarC"))
regmdc(X_design, y, s = 2L, method = "hk", V = 3.0)
regmdc(X_design, y, s = 2L, method = "emhk", V = 2.0,
       variation_constrained_covariates = c(2L))
regmdc(X_design, y, s = 2L, method = "emhk", V = 2.0,
       is_monotonically_increasing = FALSE,
       variation_constrained_covariates = c("VarB"))
regmdc(X_design, y, s = 2L, method = "emhk", V = 2.0,
       increasing_covariates = c(1L),
       variation_constrained_covariates = c(2L))
regmdc(X_design, y, s = 2L, method = "emhk", V = 2.0,
       increasing_covariates = c("VarA"),
       decreasing_covariates = c("VarC"),
       variation_constrained_covariates = c("VarB"))

fstar <- function(x) {(
  - max(x[1] - 0.25, 0) - max(x[2] - 0.25, 0)
  - max(x[1] - 0.25, 0) * max(x[2] - 0.25, 0)
)}  # the true underlying function
X_design <- cbind(runif(50), runif(50), runif(50))
colnames(X_design) <- c("VarA", "VarB", "VarC")
theta <- apply(X_design, MARGIN = 1L, FUN = fstar)
sigma <- 0.1
y <- theta + sigma * rnorm(nrow(X_design))

regmdc(X_design, y, s = 2L, method = "tc")
regmdc(X_design, y, s = 2L, method = "tc", is_totally_concave = FALSE)
regmdc(X_design, y, s = 2L, method = "tc", concave_covariates = c(1L, 2L))
regmdc(X_design, y, s = 2L, method = "tc", convex_covariates = c(3L))
regmdc(X_design, y, s = 2L, method = "tc",
       concave_covariates = c("VarA", "VarB"), convex_covariates = c("VarC"))
regmdc(X_design, y, s = 2L, method = "tc", extra_linear_covariates = c(3L))
regmdc(X_design, y, s = 2L, method = "tc", is_totally_concave = FALSE,
       extra_linear_covariates = c("VarC"))
regmdc(X_design, y, s = 2L, method = "tc", extra_linear_covariates = c(2L, 3L))
regmdc(X_design, y, s = 2L, method = "tc", concave_covariates = c("VarA"),
       extra_linear_covariates = c("VarC"))
regmdc(X_design, y, s = 2L, method = "tc", number_of_bins = 20L,
       extra_linear_covariates = c(3L))
regmdc(X_design, y, s = 2L, method = "mars", V = 3.0)
regmdc(X_design, y, s = 2L, method = "mars", V = 3.0, number_of_bins = 20L)
regmdc(X_design, y, s = 2L, method = "mars", V = 3.0,
       number_of_bins = c(10L, 20L, 20L))
regmdc(X_design, y, s = 2L, method = "mars", V = 3.0,
       number_of_bins = c(10L, 20L, NA))
regmdc(X_design, y, s = 2L, method = "mars", V = 3.0,
       number_of_bins = c(10L, 20L, NA), extra_linear_covariates = c("VarC"))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0,
       variation_constrained_covariates = c(2L))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0,
       is_totally_concave = FALSE,
       variation_constrained_covariates = c("VarB"))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0,
       concave_covariates = c(1L),
       variation_constrained_covariates = c(2L))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0,
       concave_covariates = c("VarA"),
       convex_covariates = c("VarC"),
       variation_constrained_covariates = c("VarB"))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0,
       concave_covariates = c(1L),
       variation_constrained_covariates = c(2L),
       extra_linear_covariates = c(3L))
regmdc(X_design, y, s = 2L, method = "tcmars", V = 2.0, number_of_bins = 20L,
       concave_covariates = c("VarA"),
       variation_constrained_covariates = c("VarB"),
       extra_linear_covariates = c("VarC"))
}
\references{
Ki, D. and Guntuboyina, A. (2025+). Totally Concave Regression.
Available at \url{https://arxiv.org/abs/2501.04360}.

Ki, D., Fang, B., and Guntuboyina, A. (2024). MARS via LASSO.
\emph{Annals of Statistics}, \strong{52}(3), 1102-1126.

Fang, B., Guntuboyina, A., and Sen, B. (2021). Multivariate
extensions of isotonic regression and total variation denoising via entire
monotonicity and Hardy—Krause variation. \emph{Annals of Statistics},
\strong{49}(2), 769-792.
}
